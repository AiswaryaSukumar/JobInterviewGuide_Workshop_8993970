{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "930683d7",
   "metadata": {},
   "source": [
    "# **Machine Learning Specialist Interview Study Notebook**\n",
    "\n",
    "---\n",
    "### **Overview**\n",
    "This notebook is designed to help you review and practice essential **Machine Learning** concepts for your upcoming **ML Specialist interview**. It blends clear **Markdown explanations** with **Python code scaffolding** to let you test, explore, and visualize each concept — similar to your previous workshop notebooks.\n",
    "\n",
    "---\n",
    "## **1. Setup and Imports**\n",
    "\n",
    "In this section, we import the common Python libraries used for Machine Learning experiments.\n",
    "\n",
    "```python\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score, log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "\n",
    "---\n",
    "## **2. Non-Parametric Models Practice: KNN vs Decision Tree**\n",
    "\n",
    "**Concept:**  \n",
    "Non-parametric models do not assume a fixed functional form. They are flexible and adapt to data patterns, but can easily overfit. Here, we compare **KNN** and **Decision Tree** models.\n",
    "\n",
    "```python\n",
    "# Create a simple synthetic dataset\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': np.linspace(0, 10, 100),\n",
    "    'Feature2': np.sin(np.linspace(0, 10, 100)) + np.random.normal(0, 0.2, 100),\n",
    "})\n",
    "data['Target'] = (data['Feature2'] > 0).astype(int)\n",
    "\n",
    "# Split dataset\n",
    "X = data[['Feature1', 'Feature2']]\n",
    "y = data['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "# Train Decision Tree model\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred):.3f}\")\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, tree_pred):.3f}\")\n",
    "```\n",
    "\n",
    "**Try it yourself:**  \n",
    "- Change `n_neighbors` in KNN (try 1, 3, 7) and observe accuracy.  \n",
    "- Change `max_depth` in Decision Tree and check if overfitting occurs.\n",
    "\n",
    "---\n",
    "## **3. Cross-Entropy (Log Loss) Understanding**\n",
    "\n",
    "**Concept:**  \n",
    "Cross-Entropy, or Log Loss, is used for classification models like Logistic Regression. It penalizes incorrect predictions based on their confidence.\n",
    "\n",
    "```python\n",
    "# True labels and predicted probabilities\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_prob = np.array([0.9, 0.2, 0.8, 0.4, 0.1])\n",
    "\n",
    "loss = log_loss(y_true, y_pred_prob)\n",
    "print(f\"Cross-Entropy Loss: {loss:.4f}\")\n",
    "```\n",
    "\n",
    "**Exercise:** Modify `y_pred_prob` to include higher confidence errors (e.g., change `0.1` to `0.95`) and observe how the loss increases.\n",
    "\n",
    "---\n",
    "## **4. Model Evaluation: MSE vs MAE**\n",
    "\n",
    "**Concept:**  \n",
    "Regression models are evaluated using metrics like **MSE** (Mean Squared Error) and **MAE** (Mean Absolute Error). MSE penalizes large errors more heavily.\n",
    "\n",
    "```python\n",
    "# Synthetic regression dataset\n",
    "np.random.seed(0)\n",
    "X_reg = np.linspace(0, 10, 50)\n",
    "y_reg = 3*X_reg + np.random.normal(0, 2, 50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reg.reshape(-1, 1), y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred):.3f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "```\n",
    "\n",
    "**Try this:** Add an outlier (e.g., `y_test[0] += 25`) and check how MSE and MAE respond.\n",
    "\n",
    "---\n",
    "## **5. Bias-Variance Tradeoff Visualization**\n",
    "\n",
    "**Concept:**  \n",
    "A key tradeoff in ML:  \n",
    "- High bias → underfitting  \n",
    "- High variance → overfitting  \n",
    "We visualize this using polynomial fits of increasing complexity.\n",
    "\n",
    "```python\n",
    "def plot_bias_variance_demo():\n",
    "    np.random.seed(10)\n",
    "    X = np.linspace(0, 6, 30)\n",
    "    y_true = np.sin(X)\n",
    "    y_noisy = y_true + np.random.normal(0, 0.2, X.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    degrees = [1, 3, 9]\n",
    "    for d in degrees:\n",
    "        coeffs = np.polyfit(X, y_noisy, d)\n",
    "        y_pred = np.polyval(coeffs, X)\n",
    "        plt.plot(X, y_pred, label=f'Degree {d}')\n",
    "\n",
    "    plt.scatter(X, y_noisy, color='black', s=20, label='Data')\n",
    "    plt.plot(X, y_true, 'g--', label='True Function')\n",
    "    plt.title('Bias-Variance Tradeoff: Model Complexity')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_bias_variance_demo()\n",
    "```\n",
    "\n",
    "**Reflect:**  \n",
    "- Low-degree (1) = High bias, Low variance  \n",
    "- High-degree (9) = Low bias, High variance  \n",
    "- The goal is a **balanced complexity**.\n",
    "\n",
    "---\n",
    "## **6. Self-Practice Tasks**\n",
    "\n",
    "Use these open-ended exercises to deepen your understanding:\n",
    "\n",
    "1. Adjust `K` in KNN and observe accuracy trends.\n",
    "2. Increase Decision Tree `max_depth` and visualize overfitting.\n",
    "3. Add Gaussian noise to regression targets and compare MSE vs MAE.\n",
    "4. Compute **Precision, Recall, and F1-score** for KNN predictions.\n",
    "5. Replot the bias–variance chart using more polynomial degrees.\n",
    "\n",
    "---\n",
    "### **Next Steps**\n",
    "After completing this notebook, review the **Study Guide** topics: Supervised vs Unsupervised Learning, Regression Analysis, Logistic Regression, KNN, Decision Trees, and Evaluation Metrics.\n",
    "\n",
    "This notebook is structured to resemble your shared workshop notebooks — combining conceptual explanations with practical code you can run and modify freely.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
