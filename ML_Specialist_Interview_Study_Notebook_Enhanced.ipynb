{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182e5613",
   "metadata": {},
   "source": [
    "# **Machine Learning Specialist Interview Study Notebook (Enhanced with Concept Clarifications)**\n",
    "\n",
    "---\n",
    "### **Overview**\n",
    "This notebook helps you revise essential **Machine Learning** concepts for your upcoming **ML Specialist interview**.  \n",
    "It includes clear Markdown explanations, conceptual clarifications for challenging areas, and Python code scaffolding for hands-on understanding.\n",
    "\n",
    "---\n",
    "## **1. Setup and Imports**\n",
    "\n",
    "In this section, we import the common Python libraries used for Machine Learning experiments.\n",
    "\n",
    "```python\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score, log_loss, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "\n",
    "---\n",
    "## **2. Non-Parametric Models Practice: KNN vs Decision Tree**\n",
    "\n",
    "**Concept:**  \n",
    "Non-parametric models do **not assume a predefined mathematical function** for relationships between inputs and outputs.  \n",
    "They adapt to data patterns naturally but **require larger datasets** to generalize well.  \n",
    "The more complex or flexible a model is, the more data it needs to avoid **memorizing noise** (overfitting).\n",
    "\n",
    "**Example analogy:**  \n",
    "Imagine trying to learn handwriting styles. If you see only one or two examples, you can’t generalize the pattern — you need **many examples** to understand different ways people write the same letter.\n",
    "\n",
    "```python\n",
    "# Create a simple synthetic dataset\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': np.linspace(0, 10, 100),\n",
    "    'Feature2': np.sin(np.linspace(0, 10, 100)) + np.random.normal(0, 0.2, 100),\n",
    "})\n",
    "data['Target'] = (data['Feature2'] > 0).astype(int)\n",
    "\n",
    "# Split dataset\n",
    "X = data[['Feature1', 'Feature2']]\n",
    "y = data['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "# Train Decision Tree model\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, knn_pred):.3f}\")\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, tree_pred):.3f}\")\n",
    "```\n",
    "\n",
    "**Try it yourself:**  \n",
    "- Change `n_neighbors` in KNN (try 1, 3, 7) and observe accuracy.  \n",
    "- Change `max_depth` in Decision Tree and see when it starts to overfit.\n",
    "\n",
    "---\n",
    "## **3. Cross-Entropy (Log Loss) Understanding**\n",
    "\n",
    "**Concept:**  \n",
    "Cross-Entropy (or Log Loss) measures how confident your classification model is when making predictions.  \n",
    "It **penalizes incorrect predictions more strongly when the model is overconfident**.  \n",
    "A prediction of 0.95 for a class that turns out to be wrong hurts much more than predicting 0.55.\n",
    "\n",
    "**Intuition:**  \n",
    "Think of it like being \"too sure\" — the more certain your wrong guess, the bigger the loss.\n",
    "\n",
    "```python\n",
    "# True labels and predicted probabilities\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_prob = np.array([0.9, 0.2, 0.8, 0.4, 0.1])\n",
    "\n",
    "loss = log_loss(y_true, y_pred_prob)\n",
    "print(f\"Cross-Entropy Loss: {loss:.4f}\")\n",
    "```\n",
    "\n",
    "**Exercise:** Modify `y_pred_prob` to include a high-confidence mistake (e.g., change 0.1 → 0.95) and notice how much the loss increases.\n",
    "\n",
    "---\n",
    "## **4. Model Evaluation: MSE vs MAE and Classification Metrics**\n",
    "\n",
    "**Concept:**  \n",
    "Regression tasks often use **MSE** (Mean Squared Error) and **MAE** (Mean Absolute Error).  \n",
    "However, in **classification problems**, metrics like **Precision**, **Recall**, and **F1-score** give a more realistic view — especially when one class dominates the dataset.\n",
    "\n",
    "**Simple Explanation:**  \n",
    "- **Precision:** Of the predictions you made as “positive,” how many were correct?  \n",
    "- **Recall:** Of all actual “positives,” how many did you catch?  \n",
    "- **F1-score:** A balance between Precision and Recall.\n",
    "\n",
    "```python\n",
    "# Example: Regression Evaluation\n",
    "np.random.seed(0)\n",
    "X_reg = np.linspace(0, 10, 50)\n",
    "y_reg = 3*X_reg + np.random.normal(0, 2, 50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reg.reshape(-1, 1), y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred):.3f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "```\n",
    "\n",
    "**Extra Example for Classification Metrics:**\n",
    "\n",
    "```python\n",
    "y_true = [1,0,0,0,1,0,0,0,1,0]\n",
    "y_pred = [1,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "## **5. Bias-Variance Tradeoff Visualization**\n",
    "\n",
    "**Concept:**  \n",
    "A machine learning model must balance **bias** (how simple it is) and **variance** (how sensitive it is to training data).  \n",
    "\n",
    "- **High Bias:** The model is too simple — misses patterns (underfitting).  \n",
    "- **High Variance:** The model is too complex — memorizes noise (overfitting).  \n",
    "- **Goal:** Find the sweet spot where the model generalizes well.\n",
    "\n",
    "```python\n",
    "def plot_bias_variance_demo():\n",
    "    np.random.seed(10)\n",
    "    X = np.linspace(0, 6, 30)\n",
    "    y_true = np.sin(X)\n",
    "    y_noisy = y_true + np.random.normal(0, 0.2, X.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    degrees = [1, 3, 9]\n",
    "    for d in degrees:\n",
    "        coeffs = np.polyfit(X, y_noisy, d)\n",
    "        y_pred = np.polyval(coeffs, X)\n",
    "        plt.plot(X, y_pred, label=f'Degree {d}')\n",
    "\n",
    "    plt.scatter(X, y_noisy, color='black', s=20, label='Data')\n",
    "    plt.plot(X, y_true, 'g--', label='True Function')\n",
    "    plt.title('Bias-Variance Tradeoff: Model Complexity')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_bias_variance_demo()\n",
    "```\n",
    "\n",
    "**Reflection:**  \n",
    "- Degree 1 → High bias, low variance (too simple).  \n",
    "- Degree 9 → Low bias, high variance (too complex).  \n",
    "- The middle degree provides **balanced learning**.\n",
    "\n",
    "---\n",
    "## **6. Self-Practice Tasks**\n",
    "\n",
    "Try the following hands-on challenges to reinforce understanding:\n",
    "\n",
    "1. Adjust `K` in KNN and see how model flexibility changes accuracy.  \n",
    "2. Increase `max_depth` in Decision Trees and visualize when overfitting starts.  \n",
    "3. Add noise to regression targets and compare how MSE and MAE react.  \n",
    "4. Compute **Precision, Recall, and F1-score** for your models.  \n",
    "5. Experiment with polynomial degrees (1–12) in bias-variance visualization.\n",
    "\n",
    "---\n",
    "### **Final Notes**\n",
    "- Non-parametric models = more flexible → need more data.  \n",
    "- Cross-Entropy = punishes overconfident wrong predictions.  \n",
    "- Precision/Recall/F1 = better than accuracy for imbalanced datasets.  \n",
    "- Bias–Variance = simplicity vs. complexity balance.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
